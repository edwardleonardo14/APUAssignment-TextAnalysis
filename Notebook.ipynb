{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd9dc317",
   "metadata": {},
   "source": [
    "# Edward Leonardo - TP058284"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14ae960",
   "metadata": {},
   "source": [
    "# Individual Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a35fada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text for Q1 - Q3\n",
    "text_1 = \"\"\"Sentiment analysis is \"contextual mining of text which identifies and extracts subjective information\" in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations. However, analysis of social media streams is usually restricted to just basic sentiment analysis and count based metrics. This is akin to just scratching the surface and missing out on those high value insights that are waiting to be discovered. So what should a brand do to capture that low hanging fruit?\"\"\"\n",
    "#Text for Q4\n",
    "text_2 = \"\"\"A videogame or computergame is an electronic-game that involves interaction with a user interface or input device\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef5f7f",
   "metadata": {},
   "source": [
    "## Question 1 - Form Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecabf05c",
   "metadata": {},
   "source": [
    "### 1. Demonstrate sentence segmentation and report the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12643e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentiment analysis is \"contextual mining of text which identifies and extracts subjective information\" in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations.', 'However, analysis of social media streams is usually restricted to just basic sentiment analysis and count based metrics.', 'This is akin to just scratching the surface and missing out on those high value insights that are waiting to be discovered.', 'So what should a brand do to capture that low hanging fruit?']\n",
      "\n",
      "Amount of sentences:  4\n"
     ]
    }
   ],
   "source": [
    "#import Library\n",
    "import nltk\n",
    "\n",
    "#text Input\n",
    "text_raw = text_1\n",
    "\n",
    "#sentence segmentation using NLTK\n",
    "nltk_sent_tokens = nltk.sent_tokenize(text_raw)\n",
    "\n",
    "#display results\n",
    "print(nltk_sent_tokens)\n",
    "print(\"\\nAmount of sentences: \", len(nltk_sent_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ee1044",
   "metadata": {},
   "source": [
    "### 2. Demonstrate word tokenisation using the Split Function, Regular Expression, and NLTK packages separately and report the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8231d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text Input\n",
    "text_raw = text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "991ef2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentiment', 'analysis', 'is', '\"contextual', 'mining', 'of', 'text', 'which', 'identifies', 'and', 'extracts', 'subjective', 'information\"', 'in', 'source', 'material,', 'and', 'helping', 'a', 'business', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand,', 'product', 'or', 'service', 'while', 'monitoring', 'online', 'conversations.', 'However,', 'analysis', 'of', 'social', 'media', 'streams', 'is', 'usually', 'restricted', 'to', 'just', 'basic', 'sentiment', 'analysis', 'and', 'count', 'based', 'metrics.', 'This', 'is', 'akin', 'to', 'just', 'scratching', 'the', 'surface', 'and', 'missing', 'out', 'on', 'those', 'high', 'value', 'insights', 'that', 'are', 'waiting', 'to', 'be', 'discovered.', 'So', 'what', 'should', 'a', 'brand', 'do', 'to', 'capture', 'that', 'low', 'hanging', 'fruit?']\n"
     ]
    }
   ],
   "source": [
    "#Using Python split() function\n",
    "py_word = text_raw.split()\n",
    "\n",
    "#result\n",
    "print(py_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b1452e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentiment', 'analysis', 'is', 'contextual', 'mining', 'of', 'text', 'which', 'identifies', 'and', 'extracts', 'subjective', 'information', 'in', 'source', 'material', 'and', 'helping', 'a', 'business', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand', 'product', 'or', 'service', 'while', 'monitoring', 'online', 'conversations', 'However', 'analysis', 'of', 'social', 'media', 'streams', 'is', 'usually', 'restricted', 'to', 'just', 'basic', 'sentiment', 'analysis', 'and', 'count', 'based', 'metrics', 'This', 'is', 'akin', 'to', 'just', 'scratching', 'the', 'surface', 'and', 'missing', 'out', 'on', 'those', 'high', 'value', 'insights', 'that', 'are', 'waiting', 'to', 'be', 'discovered', 'So', 'what', 'should', 'a', 'brand', 'do', 'to', 'capture', 'that', 'low', 'hanging', 'fruit']\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "import re\n",
    "\n",
    "#Using Regular Expression\n",
    "re_word = re.findall(r\"\\w+\", text_raw)\n",
    "\n",
    "#result\n",
    "print(re_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58b45d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentiment', 'analysis', 'is', '``', 'contextual', 'mining', 'of', 'text', 'which', 'identifies', 'and', 'extracts', 'subjective', 'information', \"''\", 'in', 'source', 'material', ',', 'and', 'helping', 'a', 'business', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand', ',', 'product', 'or', 'service', 'while', 'monitoring', 'online', 'conversations', '.', 'However', ',', 'analysis', 'of', 'social', 'media', 'streams', 'is', 'usually', 'restricted', 'to', 'just', 'basic', 'sentiment', 'analysis', 'and', 'count', 'based', 'metrics', '.', 'This', 'is', 'akin', 'to', 'just', 'scratching', 'the', 'surface', 'and', 'missing', 'out', 'on', 'those', 'high', 'value', 'insights', 'that', 'are', 'waiting', 'to', 'be', 'discovered', '.', 'So', 'what', 'should', 'a', 'brand', 'do', 'to', 'capture', 'that', 'low', 'hanging', 'fruit', '?']\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "import nltk\n",
    "\n",
    "#Using NLTK word tokenisation\n",
    "nltk_word = nltk.word_tokenize(text_raw)\n",
    "\n",
    "#result\n",
    "print(nltk_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9420fe61",
   "metadata": {},
   "source": [
    "### 3. Explain the differences of the tokenisation operations performed in Q1.2 using the reported output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1918b127",
   "metadata": {},
   "source": [
    "The difference between the three methods:\n",
    "1. Python split() would only do tokenisation based on the parameter input. Since the input is a whitespace, any special characters attached to a word, in the example would be an double quote, a question mark, and the period would be stuck on the word itself.\n",
    "\n",
    "2. Regular Expression findall() method allow us to do tokenisation based on their predetermined pattern. since we use \\w+, which split for each word, resulted in all special characters would be deleted. But on the other hand, Regex gives a better flexibility since the filter can be customized\n",
    "\n",
    "3. NLTK word_tokenize() method gives the best of both world, where the special characters are still available in the tokenisation result, but are separated from the words, making it easy for any next step to be taken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caea474",
   "metadata": {},
   "source": [
    "### 4. Justify the most suitable tokenisation operation for text analysis. Support your answer using obtained outputs."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a57c9756",
   "metadata": {},
   "source": [
    "From the result above, NLTK's word_tokenize() method give the best result overall. It separates the special/punctuation characters from the word its associated with, which makes it more convenient to Python's split(), but did not delete it immediately, which provides a better output from Regular Expression's findall(), since special characters should not be removed in some cases, making the NLTKâ€™s function result more versatile to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8154876",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4976738c",
   "metadata": {},
   "source": [
    "## Question 2 - Form Word Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967105a4",
   "metadata": {},
   "source": [
    "### 1. Explain the importance of stemming in text analysis."
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a923f61",
   "metadata": {},
   "source": [
    "Stemming is a Natural Language Processing (NLP) technique that lowers inflection in words to the basic, root forms. Inflection itself is a process where a word is modified to be suitable to many special grammatical situations.\n",
    "\n",
    "So, in a sense, stemming is a process to reduce any variants of a word to their basic form, even if the basic form is not a legitimate word in the language.\n",
    "\n",
    "Stemming is an important step in NLP, because having a word that exist in several inflected forms can add data redundancy to the text corpus result, rendering the NLP or ML models ineffective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5a8a25",
   "metadata": {},
   "source": [
    "### 2. Explain the differences among Regular Expression Stemmer and Porter Stemmer use for word stemming."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d945661",
   "metadata": {},
   "source": [
    "1. Porter Stemmer\n",
    "Porter Stemmer is one of the most popular stemming algorithms. Truncating algorithms aim at cutting the words into a fixed length. Porter Stemmer is a part of the nltk.stem library, PorterStemmer module. Porter Stemmer already have a fixed affixes included in the module, so no affix configuration is needed before Porter Stemmer can work \n",
    "\n",
    "2. Regular Expression Stemmer\n",
    "Regular Expression Stemmer, or Regex Stemmer is stemming technique that uses the regular expressions to identify the same affixes, where any substrings that match the pattern will be removed. this means the substring that is chosen for stemming can be modified. there is also a minimum length of string parameter that make sure any words below the minimum length will not be affected. Regex Stemmer also a part of nltk.stem library, but under the RegexpStemmer module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c04c52",
   "metadata": {},
   "source": [
    " ### 3. Demonstrate and report the output for word stemming using any techniques such as Regular Expression or Porter Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd940013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library for word tokenization\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "#text Input\n",
    "text_raw = text_1\n",
    "\n",
    "#making all text lowercase\n",
    "text_lower = text_raw.lower()\n",
    "\n",
    "#removing punctuation and special characters\n",
    "text_no_punc = re.sub(r'[^\\w\\s]+','', text_lower)\n",
    "\n",
    "#stripping trailing whitespaces\n",
    "text_no_wspace = text_no_punc.strip()\n",
    "\n",
    "#word segmentation/tokenisation\n",
    "nltk_word = nltk.word_tokenize(text_no_wspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26c2fa22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment  --->  sentiment\n",
      "analysis  --->  analysi\n",
      "is  --->  is\n",
      "contextual  --->  contextu\n",
      "mining  --->  mine\n",
      "of  --->  of\n",
      "text  --->  text\n",
      "which  --->  which\n",
      "identifies  --->  identifi\n",
      "and  --->  and\n",
      "extracts  --->  extract\n",
      "subjective  --->  subject\n",
      "information  --->  inform\n",
      "in  --->  in\n",
      "source  --->  sourc\n",
      "material  --->  materi\n",
      "and  --->  and\n",
      "helping  --->  help\n",
      "a  --->  a\n",
      "business  --->  busi\n",
      "to  --->  to\n",
      "understand  --->  understand\n",
      "the  --->  the\n",
      "social  --->  social\n",
      "sentiment  --->  sentiment\n",
      "of  --->  of\n",
      "their  --->  their\n",
      "brand  --->  brand\n",
      "product  --->  product\n",
      "or  --->  or\n",
      "service  --->  servic\n",
      "while  --->  while\n",
      "monitoring  --->  monitor\n",
      "online  --->  onlin\n",
      "conversations  --->  convers\n",
      "however  --->  howev\n",
      "analysis  --->  analysi\n",
      "of  --->  of\n",
      "social  --->  social\n",
      "media  --->  media\n",
      "streams  --->  stream\n",
      "is  --->  is\n",
      "usually  --->  usual\n",
      "restricted  --->  restrict\n",
      "to  --->  to\n",
      "just  --->  just\n",
      "basic  --->  basic\n",
      "sentiment  --->  sentiment\n",
      "analysis  --->  analysi\n",
      "and  --->  and\n",
      "count  --->  count\n",
      "based  --->  base\n",
      "metrics  --->  metric\n",
      "this  --->  thi\n",
      "is  --->  is\n",
      "akin  --->  akin\n",
      "to  --->  to\n",
      "just  --->  just\n",
      "scratching  --->  scratch\n",
      "the  --->  the\n",
      "surface  --->  surfac\n",
      "and  --->  and\n",
      "missing  --->  miss\n",
      "out  --->  out\n",
      "on  --->  on\n",
      "those  --->  those\n",
      "high  --->  high\n",
      "value  --->  valu\n",
      "insights  --->  insight\n",
      "that  --->  that\n",
      "are  --->  are\n",
      "waiting  --->  wait\n",
      "to  --->  to\n",
      "be  --->  be\n",
      "discovered  --->  discov\n",
      "so  --->  so\n",
      "what  --->  what\n",
      "should  --->  should\n",
      "a  --->  a\n",
      "brand  --->  brand\n",
      "do  --->  do\n",
      "to  --->  to\n",
      "capture  --->  captur\n",
      "that  --->  that\n",
      "low  --->  low\n",
      "hanging  --->  hang\n",
      "fruit  --->  fruit\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "#using Porter Stemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#printing result\n",
    "for word in nltk_word:\n",
    "    print(word,\" ---> \", ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0267ecc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment  --->  sentiment\n",
      "analysis  --->  analysi\n",
      "is  --->  is\n",
      "contextual  --->  contextual\n",
      "mining  --->  min\n",
      "of  --->  of\n",
      "text  --->  text\n",
      "which  --->  which\n",
      "identifies  --->  identifie\n",
      "and  --->  and\n",
      "extracts  --->  extract\n",
      "subjective  --->  subjectiv\n",
      "information  --->  information\n",
      "in  --->  in\n",
      "source  --->  sourc\n",
      "material  --->  material\n",
      "and  --->  and\n",
      "helping  --->  help\n",
      "a  --->  a\n",
      "business  --->  busines\n",
      "to  --->  to\n",
      "understand  --->  understand\n",
      "the  --->  the\n",
      "social  --->  social\n",
      "sentiment  --->  sentiment\n",
      "of  --->  of\n",
      "their  --->  their\n",
      "brand  --->  brand\n",
      "product  --->  product\n",
      "or  --->  or\n",
      "service  --->  servic\n",
      "while  --->  whil\n",
      "monitoring  --->  monitor\n",
      "online  --->  onlin\n",
      "conversations  --->  conversation\n",
      "however  --->  however\n",
      "analysis  --->  analysi\n",
      "of  --->  of\n",
      "social  --->  social\n",
      "media  --->  media\n",
      "streams  --->  stream\n",
      "is  --->  is\n",
      "usually  --->  usually\n",
      "restricted  --->  restricted\n",
      "to  --->  to\n",
      "just  --->  just\n",
      "basic  --->  basic\n",
      "sentiment  --->  sentiment\n",
      "analysis  --->  analysi\n",
      "and  --->  and\n",
      "count  --->  count\n",
      "based  --->  based\n",
      "metrics  --->  metric\n",
      "this  --->  thi\n",
      "is  --->  is\n",
      "akin  --->  akin\n",
      "to  --->  to\n",
      "just  --->  just\n",
      "scratching  --->  scratch\n",
      "the  --->  the\n",
      "surface  --->  surfac\n",
      "and  --->  and\n",
      "missing  --->  miss\n",
      "out  --->  out\n",
      "on  --->  on\n",
      "those  --->  thos\n",
      "high  --->  high\n",
      "value  --->  valu\n",
      "insights  --->  insight\n",
      "that  --->  that\n",
      "are  --->  are\n",
      "waiting  --->  wait\n",
      "to  --->  to\n",
      "be  --->  be\n",
      "discovered  --->  discovered\n",
      "so  --->  so\n",
      "what  --->  what\n",
      "should  --->  should\n",
      "a  --->  a\n",
      "brand  --->  brand\n",
      "do  --->  do\n",
      "to  --->  to\n",
      "capture  --->  captur\n",
      "that  --->  that\n",
      "low  --->  low\n",
      "hanging  --->  hang\n",
      "fruit  --->  fruit\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "from nltk import RegexpStemmer\n",
    "\n",
    "#using Regular Expression Stemmer\n",
    "#the below Regex Stemmer will only remove -ing, -s, -e, and -able, with\n",
    "# a minimum length character to be stemmed is 4 characters long\n",
    "res = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "\n",
    "#printing result\n",
    "for word in nltk_word:\n",
    "    print(word,\" ---> \", res.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec1465c",
   "metadata": {},
   "source": [
    "### 4. Justify the most suitbale stemming operation for text analytics. Support your answer using the obtained output."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e5c05117",
   "metadata": {},
   "source": [
    "For basic, most widespread use of stemming, Porter Stemmer would be the best choice for text analytics, since it is simpler to use. But on some cases where the task only needs to apply stemming on some specific suffix, Regex Stemming can be a good choice, but Regex Stemming can cause a mis stemming or over stemming, where the definition of the word is changed after the stemming. in the output above, \"mining\" becomes min in regex stemmer, which changes the meaning of the word. This is caused by the search pattern is not defined properly, meaning Regex Stemming require more preparation before being used. Meanwhile, Porter Stemmer managed to reduce it to mine, where the same meaning still holds true. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd519df",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b334c7b2",
   "metadata": {},
   "source": [
    "## Question 3 - Filter Stop Words and Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84d40db",
   "metadata": {},
   "source": [
    "### 1. Demonstrate stop words and punctuations removal from the given text corpus and report the output suitably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9bcb782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis is \"contextual mining of text which identifies and extracts subjective information\" in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations. However, analysis of social media streams is usually restricted to just basic sentiment analysis and count based metrics. This is akin to just scratching the surface and missing out on those high value insights that are waiting to be discovered. So what should a brand do to capture that low hanging fruit?\n"
     ]
    }
   ],
   "source": [
    "#text Input\n",
    "text_raw = text_1\n",
    "print(text_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "544b6791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment analysis is \"contextual mining of text which identifies and extracts subjective information\" in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations. however, analysis of social media streams is usually restricted to just basic sentiment analysis and count based metrics. this is akin to just scratching the surface and missing out on those high value insights that are waiting to be discovered. so what should a brand do to capture that low hanging fruit?\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "#get the stopwords corpus list based on the language\n",
    "stop_words = set (stopwords.words('english'))\n",
    "\n",
    "#set the text into all lowercase\n",
    "text_lower = text_raw.lower()\n",
    "print(text_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49bc3ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment analysis is contextual mining of text which identifies and extracts subjective information in source material and helping a business to understand the social sentiment of their brand product or service while monitoring online conversations however analysis of social media streams is usually restricted to just basic sentiment analysis and count based metrics this is akin to just scratching the surface and missing out on those high value insights that are waiting to be discovered so what should a brand do to capture that low hanging fruit\n"
     ]
    }
   ],
   "source": [
    "#since there are no number in the text, number removal/subtitution will be skipped\n",
    "#removing punctuation\n",
    "text_no_punc = re.sub(r'[^\\w\\s]+','',text_lower)\n",
    "print(text_no_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b352c23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment analysis is contextual mining of text which identifies and extracts subjective information in source material and helping a business to understand the social sentiment of their brand product or service while monitoring online conversations however analysis of social media streams is usually restricted to just basic sentiment analysis and count based metrics this is akin to just scratching the surface and missing out on those high value insights that are waiting to be discovered so what should a brand do to capture that low hanging fruit\n"
     ]
    }
   ],
   "source": [
    "#removing trailing whitespace\n",
    "text_no_wspace = text_no_punc.strip()\n",
    "print(text_no_wspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19411d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentiment', 'analysis', 'is', 'contextual', 'mining', 'of', 'text', 'which', 'identifies', 'and', 'extracts', 'subjective', 'information', 'in', 'source', 'material', 'and', 'helping', 'a', 'business', 'to', 'understand', 'the', 'social', 'sentiment', 'of', 'their', 'brand', 'product', 'or', 'service', 'while', 'monitoring', 'online', 'conversations', 'however', 'analysis', 'of', 'social', 'media', 'streams', 'is', 'usually', 'restricted', 'to', 'just', 'basic', 'sentiment', 'analysis', 'and', 'count', 'based', 'metrics', 'this', 'is', 'akin', 'to', 'just', 'scratching', 'the', 'surface', 'and', 'missing', 'out', 'on', 'those', 'high', 'value', 'insights', 'that', 'are', 'waiting', 'to', 'be', 'discovered', 'so', 'what', 'should', 'a', 'brand', 'do', 'to', 'capture', 'that', 'low', 'hanging', 'fruit']\n"
     ]
    }
   ],
   "source": [
    "#converting string into list\n",
    "#either using nltk word_tokenise,\n",
    "#or split() since the text has been pre-processed\n",
    "text_word_token = nltk.word_tokenize(text_no_wspace)\n",
    "print(text_word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dae5157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sentiment', 'analysis', 'contextual', 'mining', 'text', 'identifies', 'extracts', 'subjective', 'information', 'source', 'material', 'helping', 'business', 'understand', 'social', 'sentiment', 'brand', 'product', 'service', 'monitoring', 'online', 'conversations', 'however', 'analysis', 'social', 'media', 'streams', 'usually', 'restricted', 'basic', 'sentiment', 'analysis', 'count', 'based', 'metrics', 'akin', 'scratching', 'surface', 'missing', 'high', 'value', 'insights', 'waiting', 'discovered', 'brand', 'capture', 'low', 'hanging', 'fruit']\n"
     ]
    }
   ],
   "source": [
    "#removing stopwords\n",
    "text_no_stopwords = []\n",
    "stopwords_included = []\n",
    "for word in text_word_token:\n",
    "    if not word in stop_words:\n",
    "        text_no_stopwords.append(word)\n",
    "    else:\n",
    "        stopwords_included.append(word)\n",
    "    \n",
    "#result of the results after stopwords removal\n",
    "print(text_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dce902a",
   "metadata": {},
   "source": [
    "### 2. Report the stop words found in the given text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6e79cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is', 'of', 'which', 'and', 'in', 'and', 'a', 'to', 'the', 'of', 'their', 'or', 'while', 'of', 'is', 'to', 'just', 'and', 'this', 'is', 'to', 'just', 'the', 'and', 'out', 'on', 'those', 'that', 'are', 'to', 'be', 'so', 'what', 'should', 'a', 'do', 'to', 'that']\n"
     ]
    }
   ],
   "source": [
    "#result of the stopwords that has been removed\n",
    "print(stopwords_included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f1f80d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"couldn't\", 'needn', 'there', 'the', 'should', 'by', 'during', 'some', 'then', 'have', 'to', 'very', \"that'll\", 'few', 'nor', 'yours', 'further', 'we', 'any', \"you're\", 'in', 'has', 'above', 'didn', \"it's\", 'not', 's', 'won', 'down', 'herself', 'she', 'or', 'why', 'doing', 'these', 'isn', 'having', 'its', 'm', 'mustn', 'out', \"should've\", 'don', 'over', 'where', 'before', 'more', 'those', 'and', 'who', 'they', 'for', 'same', 'off', 'being', 'a', \"shouldn't\", \"won't\", 've', 'doesn', 'which', 'other', \"doesn't\", 'after', 'now', 'than', 'himself', 'our', \"she's\", 'so', 'will', 'here', 'was', \"shan't\", \"wasn't\", 'ma', 'under', 'theirs', 'be', 'both', \"mustn't\", 'been', \"isn't\", 'he', 'shouldn', 'am', 't', 'can', 'too', 'this', 'couldn', 'hadn', 'as', \"hadn't\", 'weren', \"mightn't\", 'me', 'yourselves', \"don't\", 'own', 'that', 'once', \"you'd\", 'each', 'does', 'all', 'haven', \"haven't\", 'when', 'from', \"hasn't\", 'ourselves', 'with', 'below', 'against', 'of', 're', 'what', 'him', 'just', 'are', 'did', 'while', 'their', 'into', 'hasn', 'themselves', \"wouldn't\", 'mightn', 'but', 'such', 'i', 'again', \"didn't\", 'most', 'until', 'll', 'myself', 'them', 'about', 'through', 'ours', 'no', 'how', 'hers', \"you'll\", 'itself', 'at', 'is', 'your', 'you', \"needn't\", \"weren't\", 'between', 'if', \"aren't\", 'an', 'because', 'aren', 'd', 'o', \"you've\", 'it', 'were', 'ain', 'her', 'yourself', 'up', 'wouldn', 'his', 'on', 'do', 'my', 'shan', 'whom', 'wasn', 'y', 'only', 'had'}\n"
     ]
    }
   ],
   "source": [
    "#showing the list containing all stop words corpus in english\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff132e",
   "metadata": {},
   "source": [
    "### 3. Explain the importance of filtering the stop words and punctuations in text analytics."
   ]
  },
  {
   "cell_type": "raw",
   "id": "52a724ab",
   "metadata": {},
   "source": [
    "Stop words are usually the words that does not add any value or new information to the text itself, hence during most of NLP tasks, stop words can be removed entirely to reduce the size of the model, making it more optimized.  The same case can be said about punctuations too.\n",
    "\n",
    "But stop words and punctuation filtering is dependent on the NLP task at hand, since some tasks can have better result with the inclusion of stop words and punctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de6357c",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ab5c3",
   "metadata": {},
   "source": [
    "## Question 4 - Form Parts of Speech (POS) Taggers & Syntactic Analyers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2503139c",
   "metadata": {},
   "source": [
    "### 1. Demonstrate POS tagging using NLTK POS tagger, the Regular Expression tagger and report the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d4495ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A videogame or computergame is an electronic-game that involves interaction with a user interface or input device\n"
     ]
    }
   ],
   "source": [
    "#text Input\n",
    "text_raw = text_2\n",
    "print(text_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87e9f802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "#cleaning and tokenization process\n",
    "\n",
    "#remove non-alphabetical words\n",
    "text_clean = re.sub(\"[^a-zA-Z]\", \" \", text_raw)\n",
    "\n",
    "#remove multiple whitespaces\n",
    "text_clean = \" \".join(text_clean.split())\n",
    "\n",
    "#word tokenization\n",
    "nltk_word = word_tokenize(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c490af2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('videogame', 'NN'), ('or', 'CC'), ('computergame', 'NN'), ('is', 'VBZ'), ('an', 'DT'), ('electronic', 'JJ'), ('game', 'NN'), ('that', 'WDT'), ('involves', 'VBZ'), ('interaction', 'NN'), ('with', 'IN'), ('a', 'DT'), ('user', 'JJ'), ('interface', 'NN'), ('or', 'CC'), ('input', 'NN'), ('device', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#Applying NLTK POS tagging\n",
    "nltk_pos_tag = pos_tag(nltk_word)\n",
    "\n",
    "print(nltk_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6795eb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'AT'), ('videogame', 'NN'), ('or', 'NN'), ('computergame', 'NN'), ('is', 'NNS'), ('an', 'AT'), ('electronic', 'NN'), ('game', 'NN'), ('that', 'NN'), ('involves', 'NNS'), ('interaction', 'NN'), ('with', 'NN'), ('a', 'AT'), ('user', 'NN'), ('interface', 'NN'), ('or', 'NN'), ('input', 'NN'), ('device', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#Applying Regex POS Tagging\n",
    "#https://tedboy.github.io/nlps/generated/generated/nltk.RegexpTagger.html#nltk.RegexpTagger\n",
    "\n",
    "#defining the tag by associating it with the words\n",
    "regexp_tagger = RegexpTagger(\n",
    "    [(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),   # cardinal numbers\n",
    "    (r'(The|the|A|a|An|an)$', 'AT'),   # articles\n",
    "    (r'.*able$', 'JJ'),                # adjectives\n",
    "    (r'.*ness$', 'NN'),                # nouns formed from adjectives\n",
    "    (r'.*ly$', 'RB'),                  # adverbs\n",
    "    (r'.*s$', 'NNS'),                  # plural nouns\n",
    "    (r'.*ing$', 'VBG'),                # gerunds\n",
    "    (r'.*ed$', 'VBD'),                 # past tense verbs\n",
    "    (r'.*', 'NN')                      # nouns (default)\n",
    "])\n",
    "\n",
    "regex_pos_tag = regexp_tagger.tag(nltk_word)\n",
    "\n",
    "print(regex_pos_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0147010",
   "metadata": {},
   "source": [
    "### 2. Explain the differences of the POS taggers using the output obtained in the above question"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3b331dc",
   "metadata": {},
   "source": [
    "From the results above, NLTK POS Tagging will use the NLTK POS tag that has been determined. On the other hand, regex POS tagging need to define the tag by associating it with the words, using the regular expressions. this means the user can create customised tags if necessary. also, if the word is not associated with any of the tags defined, it will use the default one. the example above is \"with\", where in NLTK, it is classified as IN (preposition/subordinating conjunction), but in Regex, since the tag is not defined, it is classified as NN (nouns), which is the default tag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd48109",
   "metadata": {},
   "source": [
    "### 3. Justify the most suitable POS tagger for text analytics. Support your answer using the output obtained."
   ]
  },
  {
   "cell_type": "raw",
   "id": "940265da",
   "metadata": {},
   "source": [
    "The most suitable POS tagging for normal text analytics would be NLTK, mainly due to its ease of use. Regex Tagging can still be useful for other cases, where the analysis requires a specialized/custom tagging. But due to its nature requiring defining the tag first, it can be redundantly more complex to do a normal text POS tagging using regex compared to NLTKâ€™s function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942e4bad",
   "metadata": {},
   "source": [
    "### 4. Draw possible parse trees for the given sentences using python codes and report the Parse Trees along with the Python Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98408852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library for creating CFG and Parse Tree\n",
    "from nltk import RegexpParser\n",
    "\n",
    "#Creating the CFG Form\n",
    "cfg_form = RegexpParser(\"\"\"\n",
    "NP: {<DT>?<JJ>*<NN>} #To extract Noun Phrases\n",
    "P: {<IN>}Â Â Â Â Â Â Â Â Â Â Â  #To extract Prepositions\n",
    "V: {<V.*>}Â Â Â Â Â Â Â Â Â Â  #To extract Verbs\n",
    "PP: {<p> <NP>}Â Â Â Â Â Â  #To extract Prepositional Phrases\n",
    "VP: {<V> <NP|PP>*}Â Â  #To extract Verb Phrases\n",
    "Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ab7ff09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK POS Tag Result:\n",
      "(S\n",
      "  (NP A/DT videogame/NN)\n",
      "  or/CC\n",
      "  (NP computergame/NN)\n",
      "  (VP (V is/VBZ) (NP an/DT electronic/JJ game/NN))\n",
      "  that/WDT\n",
      "  (VP (V involves/VBZ) (NP interaction/NN))\n",
      "  (P with/IN)\n",
      "  (NP a/DT user/JJ interface/NN)\n",
      "  or/CC\n",
      "  (NP input/NN)\n",
      "  (NP device/NN))\n"
     ]
    }
   ],
   "source": [
    "#using NLTK POS Tag's result\n",
    "nltk_cfg = cfg_form.parse(nltk_pos_tag)\n",
    "print(\"NLTK POS Tag Result:\")\n",
    "print(nltk_cfg)\n",
    "nltk_cfg.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b08ce2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regex POS Tag Result:\n",
      "(S\n",
      "  A/AT\n",
      "  (NP videogame/NN)\n",
      "  (NP or/NN)\n",
      "  (NP computergame/NN)\n",
      "  is/NNS\n",
      "  an/AT\n",
      "  (NP electronic/NN)\n",
      "  (NP game/NN)\n",
      "  (NP that/NN)\n",
      "  involves/NNS\n",
      "  (NP interaction/NN)\n",
      "  (NP with/NN)\n",
      "  a/AT\n",
      "  (NP user/NN)\n",
      "  (NP interface/NN)\n",
      "  (NP or/NN)\n",
      "  (NP input/NN)\n",
      "  (NP device/NN))\n"
     ]
    }
   ],
   "source": [
    "#using Regex POS Tag's result\n",
    "regex_cfg = cfg_form.parse(regex_pos_tag)\n",
    "print(\"Regex POS Tag Result:\")\n",
    "print(regex_cfg)\n",
    "regex_cfg.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
